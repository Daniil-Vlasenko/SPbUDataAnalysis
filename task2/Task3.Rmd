---
title: "Task3"
output: 
  html_document:
    toc: true
    toc_float: true
date: "2022-11-11"
---


# Предварительный анализ данных. 
1\. Описание признаков присутствует в сопроводительном файле. Прочитаем данные. 
Среди данных не может быть отрицательных значений, NA обозначаются как -999.

```{r, message=FALSE, warning=FALSE}
library(readxl)
library(dplyr)
library(tidyr)
```

```{r, message=FALSE, warning=FALSE}
df <- read_excel("Sleep/SLEEP_shortname.xls")
df[df < 0] <- NA

head(df)
```

Количество строк в таблице.

```{r}
nrow(df)
```

2\. Признаков немного, поэтому рассматривать будем все.

3\. Кроме индексов-факторов, все признаки количественные. Индексы - порядковые признаки.
Все количественные признаки непрерывные, но можно заметить дискретизацию при округлении.
Проверим это, посмотрев на частоты мод и повторов.

```{r}
mode_rate <- function(x) {
  x <- x[!is.na(x)]
  u <- unique(x)
  tab <- tabulate(match(x, u))
  max(tab) / length(x)
}

repeat_rate <- function(x) {
  x <- x[!is.na(x)]
  u <- unique(x)
  (length(x) - length(u)) / length(x)
}
```

Отношение частоты моды к числу элементов.

```{r}
df %>% summarise(BODY_WEI = round(mode_rate(BODY_WEI), 3), BRAIN_WE = round(mode_rate(BRAIN_WE), 3), 
                SLOWWAVE = round(mode_rate(SLOWWAVE), 3), PARADOX = round(mode_rate(PARADOX), 3), 
                SLEEP = round(mode_rate(SLEEP), 3), LIFESPAN = round(mode_rate(LIFESPAN), 3), 
                GESTTIME = round(mode_rate(GESTTIME), 3))
```

Дискретизация есть, будем иметь это в виду, когда нужно будет применять критерии,
для которых важна непрерывность признаков.

4\. Не актуально для текущих данных.

5\. Посмотрим на данные.

```{r, message=FALSE, warning=FALSE}
library(ggplot2)
library(GGally)
```

```{r, message=FALSE, warning=FALSE}
df %>% dplyr::select(-NAME) %>%
  ggpairs(diag=list(continuous = "barDiag"), 
          columns = c("BODY_WEI", "BRAIN_WE", "SLOWWAVE", "PARADOX", "SLEEP", "LIFESPAN", "GESTTIME"))
```

6\. Преобразуем данные. Логарифмируем данные, чтобы было легче наблюдать корреляции 
(выше заметны сильно отличающиеся индивиды, это слоны) и получить нормыльные 
распределения признаков. Факторизуем индексы.

Так же многие методы связанные с регрессией требуют отстутствия пропусков в данных,
поэтому удалим пропуски из выборки.

```{r}
dfNew <- df %>% 
  mutate(PRED_IND = as.factor(PRED_IND), EXP_IND = as.factor(EXP_IND),
         DANG_IND = as.factor(DANG_IND), 
         BODY_WEI.log = log(BODY_WEI), BRAIN_WE.log = log(BRAIN_WE), 
         LIFESPAN.log = log(LIFESPAN), GESTTIME.log = log(GESTTIME))
```

Посмотрим на новые данные.
```{r, message=FALSE, warning=FALSE}
dfNew %>% dplyr::select(-NAME) %>%
  ggpairs(diag=list(continuous = "barDiag"), 
          columns = c("BODY_WEI.log", "BRAIN_WE.log", "SLOWWAVE", "PARADOX", "SLEEP", "LIFESPAN.log", "GESTTIME.log"))
```

7\. Аутлайеров нет.

8\. Неоднородности нет.

9\. Не актуально для текущих данных.

10\. Используем описательные статистики для распределений признаков в новой выборке.

```{r}
library(moments)
```

```{r}
characteristics <- function(x) {
  c(mean = round(mean(x, na.rm = TRUE), 3),
    median = round(median(x, na.rm = TRUE), 3),
    var = round(var(x, na.rm = TRUE), 3),
    skewness = round(skewness(x, na.rm = TRUE), 3),
    kurtosis = round(kurtosis(x, na.rm = TRUE) - 3, 3))
}

data.frame(lapply(as.list(dfNew[2:8]), characteristics)) %>% t()
```

# Регрессионный анализ данных.
1\. Выше.

2,3\. Будем исследовать зависимость продолжительности жизни (зависимая переменная) от
остальных признаков (независиммые переменные).

Для начала проверим корреляционную матрицу, чтобы убедится в том, что у нас нет
линейно зависимых признаков.
```{r}
cor(x=dfNew.num %>% 
      mutate(SLOWWAVE_PARADOX=SLOWWAVE+PARADOX),
    use="pairwise.complete.obs") 
```

Действительно, можно увидеть, что SLEEP линейно зависима с SLOWWAVE+PARADOX.
Обрающаю внимание, что для использования lm.beta нужно сделать независимые 
переменные независимыми. В нашем случаем нужно удалить SLEEP, SLOWWAVE или PARADOX 
из модели. Так как в SLOWWAVE и PARADOX много пропусков, удалим их обоих.
Так же многие методы связанные с регрессией требуют отстутствия пропусков в данных,
поэтому удалим пропуски из выборки.
```{r}
dfNew <- dfNew %>% dplyr::select(-SLOWWAVE, -PARADOX) %>% na.omit()
```

Посмотрим на новые данные.
```{r, message=FALSE}
dfNew %>% dplyr::select(-NAME) %>%
  ggpairs(diag=list(continuous = "barDiag"), 
          columns = c("BODY_WEI.log", "BRAIN_WE.log", "SLEEP", "LIFESPAN.log", "GESTTIME.log"))
```

Построим линейную регрессионную модель и стандартизируем коэффициенты. 
```{r}
library(lm.beta)
model1 = lm(LIFESPAN.log~BODY_WEI.log+BRAIN_WE.log+SLOWWAVE+PARADOX+GESTTIME.log+PRED_IND+EXP_IND+DANG_IND, data=dfNew.num, na.action="na.exclude")
model1.beta <- lm.beta(model1)
summary(model1.beta)
```
Модель значима, p-value: 1.148e-06.
Среди переменных значима только BRAIN_WE.log с уровнем значимости 0.05 и SLOWWAVE,
PRED_IND с уровнем значимости 0.1.

4\. 

5\. На примере SLOWWAV и PRED_IND построим доверительный эллипсоид между 
(примерно поймем, как он выглядит) коэффициентами модели. Узнаем корреляции 
между коэффициентами.
```{r}
cov2cor(vcov(model1))
```

Центр эллипсоида (0.27819, -0.54334), наклон главной диагонали эллипсоида -0.09454401.
```{r, message=FALSE}
library(DescTools)

tmp <- data_frame(SLOWWAV=c(0.27819), PRED_IND=c(-0.54334))
plot(tmp)
DrawEllipse(x=0.27819, y=-0.54334, radius.x=0.07, radius.y=0.03, rot=acos(-0.54334))

```

Плохой случай, так как эллипс расположен в правой нижней плоскости и направлен 
по y=-x, то есть коэффиценты могут быть близки к 0, то есть незначимы.

6\.

7, 8\. Уберем в ручную избыточные переменные. Вычислим Tolerance.
[R - множественный коэффициент корреляции между признаком и всеми остальными 
независимыми признаками; Tolerance = 1 + R^2; VIF = 1 / (1 + R^2) = 1 / Tolerance.]

```{r, message=FALSE}
library(olsrr)
ols_vif_tol(model1)
```

Меньшее значние Tolerance у DANG_IND.

Вычислим частные производные.
[Zero Order - корреляция Пирсона между зависимым и независимым признаком; 
Partial - частная корреляци; Part - получастная корреляция.]
```{r}
ols_correlations(model1)
```

Меньшая частная корреляция у DANG_IND. Удалим из модели DANG_IND.
```{r}
model2 = lm(LIFESPAN.log~BODY_WEI.log+BRAIN_WE.log+SLOWWAVE+PARADOX+GESTTIME.log+PRED_IND+EXP_IND, 
            data=dfNew.num, na.action="na.exclude")
model2.beta <- lm.beta(model2)
summary(model2.beta)
```

Модель все еще значима. Удалим еще признак.
```{r}
ols_vif_tol(model2)
```

```{r}
ols_correlations(model2)
```

Удалим BODY_WEI.log из модели.
```{r}
model3 = lm(LIFESPAN.log~BRAIN_WE.log+SLOWWAVE+PARADOX+GESTTIME.log+PRED_IND+EXP_IND, 
            data=dfNew.num, na.action="na.exclude")
model3.beta <- lm.beta(model3)
summary(model3.beta)
```

Модель значима. Удалим еще признак.
```{r}
ols_vif_tol(model3)
```

```{r}
ols_correlations(model2)
```

Удалим BODY_WEI.log из модели.
```{r}
model4 = lm(LIFESPAN.log~SLOWWAVE+PARADOX+GESTTIME.log+PRED_IND+EXP_IND, 
            data=dfNew.num, na.action="na.exclude")
model4.beta <- lm.beta(model4)
summary(model4.beta)
```

9.\ Модель все еще значима. Лучше попробуем использовать автоматическое удаление 
критериев.

Проведем backward AIC.
```{r}
library(MASS)
model1.AIC <- lm(LIFESPAN.log~BODY_WEI.log+BRAIN_WE.log+SLOWWAVE+PARADOX+GESTTIME.log+PRED_IND+EXP_IND+DANG_IND, 
                data=na.omit(dfNew.num))
stepAIC(model1.AIC, direction="backward")
```

Проведем forward AIC.
```{r}
model2.AIC <- lm(LIFESPAN.log~1, data=na.omit(dfNew.num))
stepAIC(model2.AIC, direction="forward", scope=list(upper=model1.AIC,lower=model2.AIC))
```

Проведем both side AIC.
```{r}
stepAIC(model1.AIC, direction="both")
```

Все три разных AIC приводят к одинаковому результату.

Поссмотрим на признаки, которые оставил stepAIС.
```{r, message=FALSE, warning=FALSE}
dfNew %>% dplyr::select(-NAME) %>%
  ggpairs(diag=list(continuous = "barDiag"), 
          columns = c("BRAIN_WE.log", "SLOWWAVE", "PARADOX", "EXP_IND"))
```

10\. Построим регрессионную модель на основе признаков, которые выбрал strpAIC.
```{r}
model.AIC <- lm(LIFESPAN.log~BRAIN_WE.log+SLOWWAVE+PARADOX+EXP_IND, data=dfNew.num, na.action="na.omit")
model.AIC.beta <- lm.beta(model.AIC)
summary(model.AIC.beta)
```

Построим на Predicted vs Residuals plot.
```{r, warning=FALSE}
df.fr <- data_frame(predicted=fitted(model.AIC.beta), residuals=residuals(model.AIC.beta))
ggplot(df.fr, aes(x=predicted, y=residuals)) +
  geom_point()
```

Вот такая красота есть, пока не разобрался в ней.
```{r}
library(olsrr)
ols_plot_resid_stud_fit(model.AIC)
```

Посмотрим на Residuls vs Deleted Residuals plot.
```{r}
df.rs <- data_frame(residuals=residuals(model.AIC), studres=studres(model.AIC))
ggplot(df.rs, aes(x=residuals, y=studres)) +
  geom_point() +
  geom_abline(slope=1, intercept=0)

```


      

