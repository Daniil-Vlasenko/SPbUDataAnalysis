---
title: "Task3"
output: 
  html_document:
    toc: true
    toc_float: true
date: "2022-11-11"
---


# Предварительный анализ данных. 
Описание признаков присутствует в сопроводительном файле. Прочитаем данные. 
Среди данных не может быть отрицательных значений, NA обозначаются как -999.

```{r, message=FALSE, warning=FALSE}
library(readxl)
library(dplyr)
library(tidyr)
```

```{r, message=FALSE, warning=FALSE}
df <- read_excel("Sleep/SLEEP_shortname.xls")
df[df < 0] <- NA

head(df)
```

Признаков немного, поэтому рассматривать будем все.

Кроме индексов-факторов, все признаки количественные. Индексы - порядковые признаки.
Все количественные признаки непрерывные, но можно заметить дискретизацию при округлении.
Проверим это, посмотрев на частоты мод

```{r}
mode_rate <- function(x) {
  x <- x[!is.na(x)]
  u <- unique(x)
  tab <- tabulate(match(x, u))
  max(tab) / length(x)
}
```

Отношение частоты моды к числу элементов.
```{r}
df %>% summarise(BODY_WEI = round(mode_rate(BODY_WEI), 3), BRAIN_WE = round(mode_rate(BRAIN_WE), 3), 
                SLOWWAVE = round(mode_rate(SLOWWAVE), 3), PARADOX = round(mode_rate(PARADOX), 3), 
                SLEEP = round(mode_rate(SLEEP), 3), LIFESPAN = round(mode_rate(LIFESPAN), 3), 
                GESTTIME = round(mode_rate(GESTTIME), 3))
```

Дискретизация есть, будем иметь это в виду, когда нужно будет применять критерии,
для которых важна непрерывность признаков.

Посмотрим на данные.

```{r, message=FALSE, warning=FALSE}
library(ggplot2)
library(GGally)
```

```{r, message=FALSE, warning=FALSE}
df %>% dplyr::select(-NAME) %>%
  ggpairs(diag=list(continuous = "barDiag"), 
          columns = c("BODY_WEI", "BRAIN_WE", "SLOWWAVE", "PARADOX", "SLEEP", "LIFESPAN", "GESTTIME"))
```

Преобразуем данные. Логарифмируем данные, чтобы было легче наблюдать корреляции 
(выше заметны сильно отличающиеся индивиды, это слоны) и получить нормальные 
распределения признаков. Факторизуем индексы.
```{r}
dfNew <- df %>% 
  mutate(PRED_IND = as.factor(PRED_IND), EXP_IND = as.factor(EXP_IND),
         DANG_IND = as.factor(DANG_IND), 
         BODY_WEI.log = log(BODY_WEI), BRAIN_WE.log = log(BRAIN_WE), 
         LIFESPAN.log = log(LIFESPAN), GESTTIME.log = log(GESTTIME))
```

Посмотрим на новые данные.
```{r, message=FALSE, warning=FALSE}
dfNew %>% dplyr::select(-NAME) %>%
  ggpairs(diag=list(continuous = "barDiag"), 
          columns = c("BODY_WEI.log", "BRAIN_WE.log", "SLOWWAVE", "PARADOX", "SLEEP", "LIFESPAN.log", "GESTTIME.log"))
```

Аутлайеров нет. Неоднородности нет.

Используем описательные статистики для распределений признаков в новой выборке.
```{r}
library(moments)
```

```{r}
characteristics <- function(x) {
  c(mean = round(mean(x, na.rm = TRUE), 3),
    median = round(median(x, na.rm = TRUE), 3),
    var = round(var(x, na.rm = TRUE), 3),
    skewness = round(skewness(x, na.rm = TRUE), 3),
    kurtosis = round(kurtosis(x, na.rm = TRUE) - 3, 3))
}

data.frame(lapply(as.list(dfNew[2:8]), characteristics)) %>% t()
```

# Регрессионный анализ данных.
1\. Для начала проверим корреляционную матрицу, чтобы убедится в том, что у нас нет
линейно зависимых признаков.
```{r}
dfNew.num <- dfNew %>% 
  mutate(PRED_IND = as.numeric(PRED_IND), EXP_IND = as.numeric(EXP_IND), 
         DANG_IND = as.numeric(DANG_IND))

cor(x=dfNew.num %>% dplyr::select(-NAME) %>%
      mutate(SLOWWAVE_PARADOX=SLOWWAVE+PARADOX),
    use="pairwise.complete.obs") 
```

Можно увидеть, что SLEEP линейно зависима с SLOWWAVE+PARADOX.
Обрающаю внимание, что для использования lm.beta нужно сделать независимые 
переменные независимыми. В нашем случаем нужно удалить SLEEP, SLOWWAVE или PARADOX 
из модели. Так как в SLOWWAVE и PARADOX много пропусков удалим их обоих.
Так же многие методы связанные с регрессией требуют отстутствия пропусков в данных,
поэтому удалим пропуски из выборки.
```{r}
dfNew.o.dum <- dfNew %>% dplyr::select(-SLOWWAVE, -PARADOX) %>% na.omit()
dfNew.o.num <- dfNew.num %>% dplyr::select(-SLOWWAVE, -PARADOX) %>% na.omit()
```

Посмотрим сколько было индивидов в выборке до и после удаления.
```{r}
c(nrow(dfNew), nrow(dfNew.o.num))
```

Посмотрим на новые данные.
```{r, message=FALSE}
dfNew.o.num %>% dplyr::select(-NAME) %>%
  ggpairs(diag=list(continuous = "barDiag"), 
          columns = c("BODY_WEI.log", "BRAIN_WE.log", "SLEEP", "LIFESPAN.log", "GESTTIME.log"))
```

2,3\. Будем исследовать зависимость продолжительности жизни (зависимая переменная) от
остальных признаков (независиммые переменные).

Построим линейную регрессионную модель c dummy variables и без них и 
стандартизируем коэффициенты. 
```{r}
library(lm.beta)

model1.num = lm(LIFESPAN.log~BODY_WEI.log+BRAIN_WE.log+SLEEP+GESTTIME.log+PRED_IND+EXP_IND+DANG_IND, 
                data=dfNew.o.num)
model1.num.beta <- lm.beta(model1.num)
summary(model1.num.beta)

model1.dum = lm(LIFESPAN.log~BODY_WEI.log+BRAIN_WE.log+SLEEP+GESTTIME.log+PRED_IND+EXP_IND+DANG_IND, 
                data=dfNew.o.dum)
model1.dum.beta <- lm.beta(model1.dum)
summary(model1.dum.beta)
```
Далее будем работать с данными без dummy variables так как модель без них
более значима, p-value: 8.626e-11. Среди переменных значимы BRAIN_WE.log с 
уровнем значимости 0.001, PRED_IND с уровнем значимости 0.05 и BODY_WEI.log, 
DANG_IND с уровнем 0.1.

7, 8\. Уберем в ручную избыточные переменные. Вычислим Tolerance.
[R - множественный коэффициент корреляции между признаком и всеми остальными 
независимыми признаками; Tolerance = 1 - R^2; VIF = 1 / (1 - R^2) = 1 / Tolerance.]

```{r, message=FALSE}
library(olsrr)
ols_vif_tol(model1.num)
```

Меньшее значние Tolerance у DANG_IND.

Вычислим частные производные.
[Zero Order - корреляция Пирсона между зависимым и независимым признаком; 
Partial - частная корреляци; Part - получастная корреляция.]
```{r}
ols_correlations(model1.num)
```

Меньшая частная корреляция у GESTTIME.log. Удалим из модели сначала GESTTIME.log, 
так как он логично коррелирует с BODY_WEI.log и BRAIN_WE.log.
```{r}
model2.num = lm(LIFESPAN.log~BODY_WEI.log+BRAIN_WE.log+SLEEP+PRED_IND+EXP_IND+DANG_IND, 
                data=dfNew.o.num)
model2.num.beta <- lm.beta(model2.num)
summary(model2.num.beta)
```

Модель значима. Удалим еще признак.
```{r}
ols_vif_tol(model2.num)
```

```{r}
ols_correlations(model2.num)
```

Tolerance меньше всего у DANG_IND, а частная уорреляция меньше у SLEEP. 
Удалим DANG_IND из модели, так как в моделе уже и так 3 индекса опасности.
```{r}
model3.num = lm(LIFESPAN.log~BODY_WEI.log+BRAIN_WE.log+SLEEP+PRED_IND+EXP_IND, 
            data=dfNew.o.num)
model3.num.beta <- lm.beta(model3.num)
summary(model3.num.beta)
```

Модель значима. Удалим еще признак.
```{r}
ols_vif_tol(model3.num)
```

```{r}
ols_correlations(model3.num)
```

У BRAIN_WE.log меньший Tolerance, но частная корреляция меньше всего у SLEEP.
Удалим SLEEP из модели, так как по таблице выше этот признак не значим, в отличае 
от BRAIN_WE.log.
```{r}
model4.num = lm(LIFESPAN.log~BODY_WEI.log+BRAIN_WE.log+PRED_IND+EXP_IND, 
            data=dfNew.o.num)
model4.num.beta <- lm.beta(model4.num)
summary(model4.num.beta)
```

Модель стала еще более значима, p-value: 7.388e-14. Удалим еще признак.
```{r}
ols_vif_tol(model4.num)
```

```{r}
ols_correlations(model4.num)
```

Tolerance наименьший у BODY_WEI.log и BODY_BRAIN.log. Частная корреляция меньшая 
у BODY_WEI.log. Удалим BODY_WEI.log.
```{r}
model5.num = lm(LIFESPAN.log~BRAIN_WE.log+PRED_IND+EXP_IND, 
            data=dfNew.o.num)
model5.num.beta <- lm.beta(model5.num)
summary(model5.num.beta)
```

Модель все еще значима. Удалим еще признак.
```{r}
ols_vif_tol(model5.num)
```

```{r}
ols_correlations(model5.num)
```

Tolerance и частная корреляция меньше всего у EXP_IND. Удалим EXP_IND
```{r}
model5.num = lm(LIFESPAN.log~BRAIN_WE.log+PRED_IND, 
            data=dfNew.o.num)
model5.num.beta <- lm.beta(model5.num)
summary(model5.num.beta)
```

Модель значима, в ней осталось 2 независимые переменных. Отановимся на этом.

5\. На примере BRAIN_WE.log и PRED_IND построим доверительный эллипсоид между 
коэффициентами модели. 
```{r, message=FALSE}
library(ellipse)
plot(ellipse(model5.num, which = c(2, 3), level = 0.95), type="l")
```

Плохая доверительная область, так как оба коэффициента могут быть незначимы.

9.\ Теперь попробуем использовать автоматическое удаление критериев.

Проведем backward AIC.
```{r, message=FALSE}
library(MASS)
model1.AIC <- lm(LIFESPAN.log~BODY_WEI.log+BRAIN_WE.log+SLEEP+GESTTIME.log+PRED_IND+EXP_IND+DANG_IND, 
                data=dfNew.o.num)
stepAIC(model1.AIC, direction="backward")
```

Проведем forward AIC.
```{r}
model2.AIC <- lm(LIFESPAN.log~1, data=na.omit(dfNew.o.num))
stepAIC(model2.AIC, direction="forward", scope=list(upper=model1.AIC,lower=model2.AIC))
```

Проведем both side AIC.
```{r}
stepAIC(model1.AIC, direction="both")
```

both side AIC и backward AIC выдает луший одинаковый результат.

Поссмотрим на признаки, которые оставил stepAIС.
```{r, message=FALSE, warning=FALSE}
dfNew.o.num %>% dplyr::select(-NAME) %>%
  ggpairs(diag=list(continuous = "barDiag"), 
          columns = c("BODY_WEI.log", "BRAIN_WE.log", "SLEEP"))

dfNew.o.num %>% dplyr::select(-NAME) %>%
  ggpairs(diag=list(continuous = "barDiag"), 
          columns = c("PRED_IND", "EXP_IND", "DANG_IND"))
```

10\. Построим регрессионную модель на основе признаков, которые выбрал stepAIC 
(все без GESTTIME.log).
```{r}
model.AIC <- lm(LIFESPAN.log~BODY_WEI.log+BRAIN_WE.log+SLEEP+PRED_IND+EXP_IND+DANG_IND, 
                data=dfNew.o.num)
model.AIC.beta <- lm.beta(model.AIC)
summary(model.AIC.beta)
```
 
Проверим нормальность остатков.
```{r}
shapiro.test(residuals(model.AIC.beta))
```
С уровнем значимости 0.05 гипотеза нормальности не отвергается.

Построим на Predicted vs Residuals plot и еще отдельно посмотрим на 
стьюдентезированные остатки.
```{r}
library(olsrr)
ols_plot_resid_stud_fit(model.AIC)
ols_plot_resid_stud(model.AIC)
```

За 3 сигмы нет выбросов, есть только 5 выбросов за 2 сигмы. Посмотрим, что
это за индивиды.
```{r}
dfNew.o.num[c(6,14,22,27,49),]$NAME
dfNew.o.num[c(6,14,22,27,49),]
```

Большая летучая коричневая мышь, ехидна, суслик, маленькая летучая коричневая мышь, тупайи.

11\. Посмотрим на Residuls vs Deleted Residuals plot.
Красная линия - это линия y=x, а синия линий - это линия регрессии.
```{r, message=FALSE, warning=FALSE}
df.rs <- data_frame(residuals=residuals(model.AIC), studres=studres(model.AIC))
ggplot(df.rs, aes(x=residuals, y=studres)) +
  geom_point() +
  geom_abline(slope=1, intercept=0, color='#E41A1C') +
  geom_smooth(method='lm')
```

Выбросов относительно регрессии нет.

11\. Растояние Махаланобиса.
```{r, message=FALSE, warning=FALSE}
library(rstatix)
mahal <- dfNew.o.num %>% 
         dplyr::select(-NAME) %>%
         mahalanobis_distance()
plot(mahal$mahal.dist)
```

Посмотрим на топ 5 индивидов.
```{r}
dfNew.o.num[mahal$mahal.dist %in% tail(sort(mahal$mahal.dist),5),]
```


Растояние Кука.
```{r, message=FALSE, warning=FALSE}
cook <- cooks.distance(model.AIC)
plot(cook)
```

Посмотрим на топ 5 индивидов.
```{r}
dfNew.o.num[cook %in% tail(sort(cook),5),]
```

